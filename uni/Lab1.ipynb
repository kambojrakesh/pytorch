{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBloGFmF-6fP",
    "outputId": "bcf98e8f-05a7-4592-c689-0be7618c9306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-mGIsER-8R2",
    "outputId": "cb52eb0f-5d72-4677-824a-c13efae910f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m841.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.9.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==0.8.0\n",
      "  Downloading torchaudio-0.8.0-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.5.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
      "Installing collected packages: torch, torchvision, torchtext, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1+cu116\n",
      "    Uninstalling torch-1.13.1+cu116:\n",
      "      Successfully uninstalled torch-1.13.1+cu116\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.14.1+cu116\n",
      "    Uninstalling torchvision-0.14.1+cu116:\n",
      "      Successfully uninstalled torchvision-0.14.1+cu116\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.14.1\n",
      "    Uninstalling torchtext-0.14.1:\n",
      "      Successfully uninstalled torchtext-0.14.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.13.1+cu116\n",
      "    Uninstalling torchaudio-0.13.1+cu116:\n",
      "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
      "Successfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchtext-0.9.0 torchvision-0.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRkN2rTp_D4w",
    "outputId": "2195b737-072f-4772-86a7-d021cfccea43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.4\n",
      "    Uninstalling spacy-3.4.4:\n",
      "      Successfully uninstalled spacy-3.4.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed spacy-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6c7ZM24_Ho5"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "#from torchtext.legacy import data\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78QfZZZW_MsZ",
    "outputId": "87d95561-ecaa-4519-d392-b05dc49b396c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1Z93DBz_Mu3",
    "outputId": "90c6cf5b-3055-4063-e6ae-8e860b0d93e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy.cli\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzmRYd9h_Mxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import fr_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_fr = fr_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNbrG2OY_ZnH"
   },
   "outputs": [],
   "source": [
    "### LATEST TORCHTEXT ###\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "spacy_en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "spacy_fr_tokenizer = get_tokenizer(\"spacy\", language=\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhMkgxxj_df-"
   },
   "outputs": [],
   "source": [
    "### LATEST TORCTHEXT ###\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import io\n",
    "\n",
    "path = './drive/My Drive/MachineTranslation/eng-fre/'\n",
    "train_fn = 'train_eng_fre.tsv'\n",
    "valid_fn = 'val_eng_fre.tsv'\n",
    "test_fn = 'test_eng_fre.tsv'\n",
    "\n",
    "\n",
    "def build_vocab(filepath, src_tokenizer, trg_tokenizer):\n",
    "  src_counter, trg_counter = Counter(), Counter()\n",
    "  with open(filepath, encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "      if i == 0:  # skip header\n",
    "        continue\n",
    "      # split line and tokenize accordingly\n",
    "      trg_line, src_line = line.strip(\"\\n\").split(\"\\t\")\n",
    "      src_counter.update(src_tokenizer(src_line.lower()))\n",
    "      trg_counter.update(trg_tokenizer(trg_line.lower()))\n",
    "    \n",
    "    # sort and wrap as OrderedDict\n",
    "    # ordered_src = OrderedDict(sorted(src_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    # ordered_trg = OrderedDict(sorted(trg_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    ordered_src = sorted(src_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_trg = sorted(trg_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # build vocab objects\n",
    "    # NOTE: OrderedDict as input Requires torchtext >= 0.10.0. Using Counter for now\n",
    "    src_vocab = Vocab(\n",
    "      src_counter, \n",
    "      min_freq=2, \n",
    "      specials=('<unk>', '<pad>', '<bos>', '<eos>')\n",
    "    )\n",
    "\n",
    "    trg_vocab = Vocab(\n",
    "      trg_counter, \n",
    "      min_freq=2,\n",
    "      specials=('<unk>', '<pad>', '<bos>', '<eos>')\n",
    "    )\n",
    "    \n",
    "    return src_vocab, trg_vocab\n",
    "\n",
    "src_vocab, trg_vocab = build_vocab(\n",
    "  path + train_fn, \n",
    "  spacy_fr_tokenizer,\n",
    "  spacy_en_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53bm3-uD_hg7"
   },
   "outputs": [],
   "source": [
    "### LATEST TORCHTEXT ###\n",
    "\n",
    "import io\n",
    "\n",
    "def data_process(path, split):\n",
    "  raw_iter = iter(io.open(path + split, encoding=\"utf-8\"))\n",
    "  data = []\n",
    "  for i, item in enumerate(raw_iter):\n",
    "    if i == 0:\n",
    "      continue\n",
    "    trg_raw, src_raw = item.strip(\"\\n\").split(\"\\t\")\n",
    "    src_tensor = torch.tensor(\n",
    "        [src_vocab[token] for token in spacy_fr_tokenizer(src_raw.lower())],\n",
    "        dtype=torch.long\n",
    "      )\n",
    "    trg_tensor = torch.tensor(\n",
    "        [trg_vocab[token] for token in spacy_en_tokenizer(trg_raw.lower())],\n",
    "        dtype=torch.long\n",
    "      )\n",
    "    data.append((src_tensor, trg_tensor))\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiIZqzM6_hja"
   },
   "outputs": [],
   "source": [
    "train_data = data_process(path, train_fn)\n",
    "val_data = data_process(path, valid_fn)\n",
    "test_data = data_process(path, test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQv9hD3z_hls",
    "outputId": "25109d58-ac1d-479c-9f28-85689344b237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(val_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQTPKEpI_hoA",
    "outputId": "c7d0fa0b-806d-4813-feb9-c90571e8c770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'] ['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '.']\n"
     ]
    }
   ],
   "source": [
    "trg_sent = [trg_vocab.itos[i] for i in train_data[0][1]]\n",
    "src_sent = [src_vocab.itos[i] for i in train_data[0][0]]\n",
    "print(trg_sent, src_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8f_ZkU2_hqR",
    "outputId": "41c74d05-4fd9-4c08-d16c-599cfdf52b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.'] ['un', 'homme', 'âgé', 'en', 'surpoids', 'fait', 'sauter', 'une', 'crêpe', 'en', 'préparant', 'le', 'petit', 'déjeuner', '.']\n"
     ]
    }
   ],
   "source": [
    "trg_sent = [trg_vocab.itos[i] for i in val_data[100][1]]\n",
    "src_sent = [src_vocab.itos[i] for i in val_data[100][0]]\n",
    "print(trg_sent, src_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vN1svypx_hs1",
    "outputId": "7929b873-84b7-4913-9b4f-000e27e68516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (fr) vocabulary: 6471\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "### LATEST TORCHTEXT ###\n",
    "\n",
    "print(f\"Unique tokens in source (fr) vocabulary: {len(src_vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(trg_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReEFTFwG_hu3",
    "outputId": "3dfc82fb-1a3b-4ddd-9172-d1c88b8db2ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# trg_stoi = trg_vocab.get_stoi()  # torcthext >0.10.0\n",
    "trg_stoi = trg_vocab.stoi\n",
    "print(trg_stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uKsBM_tAApa"
   },
   "outputs": [],
   "source": [
    "### LATEST TORCHTEXT ###\n",
    "\n",
    "BATCH_SIZE = {\n",
    "    \"train\": 16,\n",
    "    \"val\": 256,\n",
    "    \"test\": 256\n",
    "}\n",
    "\n",
    "PAD_IDX = trg_vocab['<pad>']\n",
    "BOS_IDX = trg_vocab['<bos>']\n",
    "EOS_IDX = trg_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  src_batch, trg_batch = [], []\n",
    "  for (src_item, trg_item) in data_batch:\n",
    "    src_batch.append(torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    trg_batch.append(torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "  trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "  return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8cbV41EAEEz"
   },
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE[\"train\"],\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE[\"val\"],\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE[\"test\"],\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNd-xZ7iAGa7",
    "outputId": "9cd8a0b7-a012-441a-8998-0c9cc9e6fa63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size of source language: torch.Size([25, 16])\n",
      "tensor size of target language: torch.Size([24, 16])\n",
      "the tensor of first example in target language: tensor([  2,   7, 224, 106, 141,   7,  69,   5,   3,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n"
     ]
    }
   ],
   "source": [
    "# batch example of training data\n",
    "for batch in train_iter:\n",
    "    src, trg = batch\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    print('the tensor of first example in target language:', trg[:, 0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-KCOtDaAI87"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55IxTlicAMhh"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "             \n",
    "        # input is of shape [batch_size]\n",
    "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
    "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]    \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # sequence_len and num_directions will always be 1 in the decoder.\n",
    "        # output shape is [1, batch_size, hidden_dim]\n",
    "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
    "        # predicted shape is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyqws4r4AQaP"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src is of shape [src_sequence_len, batch_size]\n",
    "        # trg is of shape [targ_sequence_len, batch_size]\n",
    "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # to store the outputs of the decoder\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
    "        enc_outputs, cell = self.encoder(src)\n",
    "        #print(enc_outputs.shape)\n",
    "        hidden = torch.mean(enc_outputs, 0).unsqueeze(0)\n",
    "        #print(hidden.shape)\n",
    "        # here we dissect the cell tensor to get fwd and bwd passes\n",
    "        fwd_cell = cell[0, :, :]\n",
    "        bwd_cell = cell[1, :, :]\n",
    "        # here we concatenate those two tensors to get\n",
    "        # the cell tensor we need with shape [1, 16, 1024]\n",
    "        cell = torch.cat((fwd_cell, bwd_cell), 1)\n",
    "        cell = cell.unsqueeze(0)\n",
    "        #print(cell.shape)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            # pick a random number between 0 to ratio and decide whether to teacher force\n",
    "            # if the ratio is 1.0, use_teacher_force is always 1 \n",
    "            # if the ratio is 0.0, use_teacher_force is always 0\n",
    "            # if the ration is 0.4, use_teacher_force is 1 for 40% of the time (on an average)\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio \n",
    "            top1 = output.max(1)[1]\n",
    "            # decide the next token based on use_teacher_force]\n",
    "            # if teacher forcing is on, the next input will be the gold token from the previous timestep\n",
    "            # otherwise, the next input will be the predicted token from the previous timestep.\n",
    "            input = (trg[t] if use_teacher_force else top1) \n",
    "\n",
    "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MktOn_JRATkk",
    "outputId": "176b42ca-0b11-40d4-f36b-0d3e63165729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X3wdkYuAV0R",
    "outputId": "416088fa-6061-49b1-d583-def1e86f8a1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "#INPUT_DIM = len(SRC.vocab) # tokens in source vocabulary\n",
    "#OUTPUT_DIM = len(TRG.vocab) # tokens in target vocabulary\n",
    "INPUT_DIM = len(src_vocab) # tokens in source vocabulary\n",
    "OUTPUT_DIM = len(trg_vocab) # tokens in target vocabulary\n",
    "\n",
    "#INPUT_DIM = 6004\n",
    "#OUTPUT_DIM = 6004\n",
    "\n",
    "# hyperparameters\n",
    "ENC_EMB_DIM = 256 # encoder embedding size\n",
    "DEC_EMB_DIM = 256 # decoder embedding size\n",
    "ENC_HID_DIM = 512 # encoder hidden size\n",
    "DEC_HID_DIM = 1024 # decoder hidden size\n",
    "ENC_DROPOUT = 0.5 # dropout for encoder\n",
    "DEC_DROPOUT = 0.3 # dropout for decoder\n",
    "N_LAYERS = 1 # number of LSTM layers\n",
    "LEARNING_RT = 0.001 # learning rate\n",
    "\n",
    "# model\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eST2YkB-AWQ0",
    "outputId": "c78886a3-5b6a-4464-af93-12fc987faa3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6471, 256)\n",
       "    (lstm): LSTM(256, 512, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (lstm): LSTM(256, 1024, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=1024, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7zBEVONAWTQ",
    "outputId": "ee17f2b6-2689-46be-adea-174421743b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17,610,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6leXgauIAWVi"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVhdBIXhAWXv",
    "outputId": "5dd94a5d-bae5-416f-ec45-289d53897e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n"
     ]
    }
   ],
   "source": [
    "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ', PAD_IDX)\n",
    "## we will ignore the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMvYtgRcAjap",
    "outputId": "8077f6fe-a0b1-4d84-d393-6a579c346514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5426, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "model.train()\n",
    "\n",
    "for i, (src, trg) in enumerate(train_iter):\n",
    "    \n",
    "    # read the source sentence and target sentence\n",
    "    # src = batch.SRC\n",
    "    # trg = batch.TRG\n",
    "    src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "    # clear the gradient buffer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output = model(src, trg)\n",
    "    #trg = [trg len, batch size]\n",
    "    #output = [trg len, batch size, output dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim)\n",
    "    trg = trg[1:].view(-1)\n",
    "\n",
    "    #trg = [(trg len - 1) * batch size]\n",
    "    #output = [(trg len - 1) * batch size, output dim]\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = criterion(output, trg)\n",
    "    \n",
    "    # compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the gradients to prevent gradient explosion problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    \n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss/src.shape[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgBiGCfWAmRP"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        \n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # loss function works only 2d logits, 1d targets\n",
    "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
    "        # trg shape shape should be [(sequence_len - 1) * batch_size]\n",
    "        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDoF288pA-J4"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) # turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPrnb1rSBGPd"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snA36u4oBJJk",
    "outputId": "78951e3c-dba0-4d35-c1d5-778f24938988"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 57s\n",
      "\tTrain Loss: 4.301 | Train PPL:  73.763\n",
      "\t Val. Loss: 4.517 |  Val. PPL:  91.584\n",
      "Epoch: 02 | Time: 1m 57s\n",
      "\tTrain Loss: 3.657 | Train PPL:  38.745\n",
      "\t Val. Loss: 4.188 |  Val. PPL:  65.863\n",
      "Epoch: 03 | Time: 1m 56s\n",
      "\tTrain Loss: 3.245 | Train PPL:  25.649\n",
      "\t Val. Loss: 3.929 |  Val. PPL:  50.844\n",
      "Epoch: 04 | Time: 1m 56s\n",
      "\tTrain Loss: 2.924 | Train PPL:  18.624\n",
      "\t Val. Loss: 3.864 |  Val. PPL:  47.672\n",
      "Epoch: 05 | Time: 1m 55s\n",
      "\tTrain Loss: 2.656 | Train PPL:  14.237\n",
      "\t Val. Loss: 3.736 |  Val. PPL:  41.939\n",
      "Epoch: 06 | Time: 1m 55s\n",
      "\tTrain Loss: 2.436 | Train PPL:  11.430\n",
      "\t Val. Loss: 3.722 |  Val. PPL:  41.341\n",
      "Epoch: 07 | Time: 1m 55s\n",
      "\tTrain Loss: 2.220 | Train PPL:   9.207\n",
      "\t Val. Loss: 3.800 |  Val. PPL:  44.696\n",
      "Epoch: 08 | Time: 1m 55s\n",
      "\tTrain Loss: 2.015 | Train PPL:   7.500\n",
      "\t Val. Loss: 3.786 |  Val. PPL:  44.060\n",
      "Epoch: 09 | Time: 1m 55s\n",
      "\tTrain Loss: 1.829 | Train PPL:   6.228\n",
      "\t Val. Loss: 3.878 |  Val. PPL:  48.310\n",
      "Epoch: 10 | Time: 1m 55s\n",
      "\tTrain Loss: 1.654 | Train PPL:   5.226\n",
      "\t Val. Loss: 3.893 |  Val. PPL:  49.046\n",
      "Epoch: 11 | Time: 1m 55s\n",
      "\tTrain Loss: 1.509 | Train PPL:   4.521\n",
      "\t Val. Loss: 3.988 |  Val. PPL:  53.948\n",
      "Epoch: 12 | Time: 1m 56s\n",
      "\tTrain Loss: 1.368 | Train PPL:   3.928\n",
      "\t Val. Loss: 4.072 |  Val. PPL:  58.651\n",
      "Epoch: 13 | Time: 1m 56s\n",
      "\tTrain Loss: 1.246 | Train PPL:   3.477\n",
      "\t Val. Loss: 4.179 |  Val. PPL:  65.287\n",
      "Epoch: 14 | Time: 1m 56s\n",
      "\tTrain Loss: 1.131 | Train PPL:   3.100\n",
      "\t Val. Loss: 4.309 |  Val. PPL:  74.345\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"./drive/My Drive/MachineTranslation/seq2seq_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKo4SCvSBLxW",
    "outputId": "628f0be7-0045-48cb-f839-af0559d1a618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INPUT_DIM = len(SRC_saved.vocab)\n",
    "# OUTPUT_DIM = len(TRG_saved.vocab)\n",
    "INPUT_DIM = len(src_vocab) # tokens in source vocabulary\n",
    "OUTPUT_DIM = len(trg_vocab) # tokens in target vocabulary\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 1024\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model_best = Seq2Seq(enc, dec, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cie0KzXx0_UK"
   },
   "outputs": [],
   "source": [
    "model_best.load_state_dict(torch.load('./drive/My Drive/MachineTranslation/seq2seq_6.pt')['state_dict'])\n",
    "model_best = model_best.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evZ9RrZ-20fm",
    "outputId": "5aaee75d-685b-447c-f5d3-cefc482051f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['Une', 'femme', 'avec', 'un', 'gros', 'sac', 'passe', 'par', 'une', 'porte', '.']\n",
      "shape of source language:  torch.Size([13, 1])\n"
     ]
    }
   ],
   "source": [
    "### LATEST TORCHTEXT ###\n",
    "\n",
    "model_best.eval()\n",
    "src_token = spacy_fr_tokenizer('Une femme avec un gros sac passe par une porte.')\n",
    "print(\"src_token:\", src_token)\n",
    "src_tensor = torch.tensor([src_vocab[token.lower()] for token in src_token], dtype=torch.long)\n",
    "src_tensor = torch.cat([torch.tensor([BOS_IDX]), src_tensor, torch.tensor([EOS_IDX])], dim=0)\n",
    "src_tensor = src_tensor.unsqueeze(1).to(device)\n",
    "print(\"shape of source language: \", src_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEb0klXR236E",
    "outputId": "f6fc826c-a127-4676-d18d-ce4caf0d5c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target language:  torch.Size([66, 1])\n"
     ]
    }
   ],
   "source": [
    "### LATEST TORCHTEXT ###\n",
    "trg_token = ['<pad>']*64\n",
    "trg_tensor = torch.tensor([trg_vocab[token.lower()] for token in trg_token], dtype=torch.long)\n",
    "trg_tensor = torch.cat([torch.tensor([BOS_IDX]), trg_tensor, torch.tensor([EOS_IDX])], dim=0)\n",
    "trg_tensor = trg_tensor.unsqueeze(1).to(device)\n",
    "print(\"shape of target language: \", trg_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9qHumca27Xa",
    "outputId": "a9babb52-ff37-41fc-851d-328a80f5b7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output translate:  torch.Size([65, 1, 5893])\n"
     ]
    }
   ],
   "source": [
    "output = model_best(src_tensor, trg_tensor, teacher_forcing_ratio = 0.0)\n",
    "output_dim = output.shape[-1]\n",
    "# get translation results, we ignore first token <sos> in both translation and target sentences. \n",
    "# output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary. \n",
    "output_translate = output[1:]\n",
    "print(\"shape of output translate: \", output_translate.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_L21HvT2_T_",
    "outputId": "55b70592-233f-4f57-ed68-3cf43e939503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token indices in source language:  [  2   6  19  15   4 185 149 212  65   6 109   5   3]\n",
      "shape of logits for each prediction token in target language torch.Size([65, 5893])\n"
     ]
    }
   ],
   "source": [
    "source_language_token_ids = src_tensor[:,0].cpu().numpy()\n",
    "print(\"token indices in source language: \", source_language_token_ids)\n",
    "translation_logit = output_translate[:,0,:].squeeze(1)\n",
    "print(\"shape of logits for each prediction token in target language\", translation_logit.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfiyBG003DZu",
    "outputId": "cfb3dc4c-3d7b-4ad7-8962-6f362849dd1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of unnormalized logits corresponding to the top prediction for each prediction token =  torch.Size([65, 1])\n",
      "token indices in target language:  [   4   14   13    4   59  645   10   41   49    4 1024    5    3    3\n",
      "    3    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3]\n"
     ]
    }
   ],
   "source": [
    "# Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "prob, token_id = translation_logit.data.topk(1)\n",
    "print(\"shape of unnormalized logits corresponding to the top prediction for each prediction token = \", prob.size())\n",
    "target_language_token_ids_along_with_pad = token_id.squeeze(1).cpu().numpy()\n",
    "print(\"token indices in target language: \", target_language_token_ids_along_with_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0lpZFa03Gux",
    "outputId": "ca172ea2-a518-4466-dd10-9dcb28955ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source language: ['<bos>', 'une', 'femme', 'avec', 'un', 'gros', 'sac', 'passe', 'par', 'une', 'porte', '.']\n",
      "Our model translation:  a woman with a large purse is walking by a gate .\n",
      "Gold translation:  a woman with a large purse is walking by a gate.\n"
     ]
    }
   ],
   "source": [
    "# get source langauge in text\n",
    "src_language_token_strs = []\n",
    "for i in source_language_token_ids:\n",
    "    #if i == SRC.vocab.stoi['<eos>']:\n",
    "    if i == src_vocab.stoi['<eos>']:\n",
    "        break\n",
    "    else:\n",
    "        #token = SRC.vocab.itos[i]\n",
    "        token = src_vocab.itos[i]\n",
    "        src_language_token_strs.append(token)\n",
    "print(\"Source language:\", src_language_token_strs)\n",
    "\n",
    "# get machine translation in text\n",
    "trans_language = []\n",
    "for i in target_language_token_ids_along_with_pad:\n",
    "    #if i == TRG.vocab.stoi['<eos>']:\n",
    "    if i == trg_vocab.stoi['<eos>']:\n",
    "        break\n",
    "    else:\n",
    "        #token = TRG.vocab.itos[i]\n",
    "        token = trg_vocab.itos[i]\n",
    "        trans_language.append(token)\n",
    "print(\"Our model translation: \",  ' '.join(trans_language))\n",
    "print(\"Gold translation: \", \"a woman with a large purse is walking by a gate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJBLPTa63KKJ"
   },
   "outputs": [],
   "source": [
    "def inference(model, file_name, src_vocab, trg_vocab, attention=False, max_trg_len=64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.legacy.data import TabularDataset\n",
    "    from torchtext.legacy.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('TRG', trg_vocab), ('SRC', src_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "        dataset=test, # we pass in the datasets we want the iterator to draw data from\n",
    "        sort=False,\n",
    "        batch_size=128,\n",
    "        sort_key=None,\n",
    "        shuffle=False,\n",
    "        sort_within_batch=False,\n",
    "        device=device,\n",
    "        train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "            trg_placeholder = trg_placeholder.long().to(device)\n",
    "            if attention == True:\n",
    "                output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            else:\n",
    "                #original \n",
    "                #output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "                \n",
    "                # update:\n",
    "                output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "            output_translate = output[1:]\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:, j])\n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:, j])\n",
    "\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u79viqi68GL"
   },
   "outputs": [],
   "source": [
    "def data_process(path):\n",
    "  raw_iter = iter(io.open(path, encoding=\"utf-8\"))\n",
    "  data = []\n",
    "  for i, item in enumerate(raw_iter):\n",
    "    if i == 0:\n",
    "      continue\n",
    "    trg_raw, src_raw = item.strip(\"\\n\").split(\"\\t\")\n",
    "    src_tensor = torch.tensor(\n",
    "        [src_vocab[token] for token in spacy_fr_tokenizer(src_raw.lower())],\n",
    "        dtype=torch.long\n",
    "      )\n",
    "    trg_tensor = torch.tensor(\n",
    "        [trg_vocab[token] for token in spacy_en_tokenizer(trg_raw.lower())],\n",
    "        dtype=torch.long\n",
    "      )\n",
    "    data.append((src_tensor, trg_tensor))\n",
    "\n",
    "  return data\n",
    "\n",
    "def inference(model, file_name, src_vocab, trg_vocab, attention=False, max_trg_len=64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "    test_data = data_process(file_name)\n",
    "    test_iter = DataLoader(test_data, batch_size=BATCH_SIZE[\"test\"],\n",
    "                       shuffle=True, collate_fn=generate_batch)\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    #TRG_PAD_IDX = trg_vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (src, trg) in enumerate(test_iter):\n",
    "\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) # turn off teacher forcing\n",
    "\n",
    "\n",
    "            output_translate = output[1:]\n",
    "            translation_logit = output_translate[:,0,:].squeeze(1)\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            target_language_token_ids_along_with_pad = token_id.squeeze(1).cpu().numpy()\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    #print(all_trg[0].shape)\n",
    "    #print(all_translated_trg[0].shape)\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        #print(cur_translation)\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:, j])\n",
    "            #print(gold_convered_strings)\n",
    "            \n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
    "            #print(trans_convered_strings)\n",
    "            #print()\n",
    "            #print('xxxxxxx---------------------xxxxx')\n",
    "            #print()\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w63KUk_Y8Lyn",
    "outputId": "c708c753-5852-4f5f-b18d-3c9e8dbcf38b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2081102059159077"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(model_best, \"./drive/My Drive/MachineTranslation/eng-fre/test_eng_fre.tsv\", src_vocab, trg_vocab, True, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7ouFBax9Fb5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
