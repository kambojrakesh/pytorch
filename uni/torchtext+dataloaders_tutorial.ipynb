{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Torchtext +Dataloaders Tutorial**\n",
    "\n",
    "#### Requirements\n",
    "* Python >=3.7, <=3.10\n",
    "* PyTorch >=1.13.0\n",
    "* Torchtext >= 0.13.0\n",
    "* Torchdata >= 0.5\n",
    "* SpaCy's [`en_core_web_sm`](https://spacy.io/models/en) and [`de_core_news_sm`](https://spacy.io/models/de) models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Torchtext**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "Two young, White males are outside near many bushes.\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_iter = iter(Multi30k(split='train'))\n",
    "de_sent, eng_sent = next(train_iter)\n",
    "print(de_sent)\n",
    "print(eng_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer\n",
    "\n",
    "The `torchtext.data.utils` module comes with the `get_tokenizer` feature, which allows loading different tokenizers depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(eng_sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_tokenizer` can take any of the following arguments:\n",
    "- If `None`, it returns the `split()` function in Python.\n",
    "- If `basic_english`, it normalizes and splits the sentence (see [source](https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer))\n",
    "- If a callable function (e.g., `def mytokenizer`), it returns the function\n",
    "- If a tokenizer library (e.g., `spacy`), it returns the corresponding library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vikki\\anaconda3\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "spacy_en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "tokens = spacy_en_tokenizer(eng_sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy comes with multilingual support, so let's try tokenizing our target sentence in german:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n"
     ]
    }
   ],
   "source": [
    "spacy_de_tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "tokens = spacy_de_tokenizer(de_sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TWO', 'YOUNG,', 'WHITE', 'MALES', 'ARE', 'OUTSIDE', 'NEAR', 'MANY', 'BUSHES.']\n"
     ]
    }
   ],
   "source": [
    "def mytokenizer(sent):\n",
    "    return sent.upper().split()\n",
    "\n",
    "my_tokenizer = get_tokenizer(mytokenizer)\n",
    "tokens = my_tokenizer(eng_sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also generate n-gram sequences using `ngram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TWO',\n",
       " 'YOUNG,',\n",
       " 'WHITE',\n",
       " 'MALES',\n",
       " 'ARE',\n",
       " 'OUTSIDE',\n",
       " 'NEAR',\n",
       " 'MANY',\n",
       " 'BUSHES.',\n",
       " 'TWO YOUNG,',\n",
       " 'YOUNG, WHITE',\n",
       " 'WHITE MALES',\n",
       " 'MALES ARE',\n",
       " 'ARE OUTSIDE',\n",
       " 'OUTSIDE NEAR',\n",
       " 'NEAR MANY',\n",
       " 'MANY BUSHES.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "list(ngrams_iterator(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced Tokenizers\n",
    "\n",
    "Torchtext offers support to incorporate more sophisticated tokernizers into our workflows (again, depending on our use case!). Let's start with the [SentencePiece](https://github.com/google/sentencepiece) tokenizer. This tokenizer leverages two segmentation algorithms - [BytePair Encoding (BPE)](http://www.aclweb.org/anthology/P16-1162) and [unigram language model](https://arxiv.org/abs/1804.10959) - and is an effective way to address the open vocabulary problem often frequent in Neural Machine Translation (and other NLP tasks).\n",
    "\n",
    "Furthermore, since `SentencePiece` treats the sentences just as sequences of Unicode characters, there is no language-dependent logic, which makes it practical for multilingual tokenization tasks. [see docs](https://github.com/google/sentencepiece), [see paper](https://aclanthology.org/D18-2012/).\n",
    "\n",
    "* Note: If you're interested in a deep dive on SentencePiece, here's a [good resource](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15) to consult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁Two', '▁young', ',', '▁White', '▁male', 's', '▁are', '▁outside', '▁near', '▁many', '▁bu', 'shes', '.'], ['▁Zwei', '▁junge', '▁weiß', 'e', '▁Männer', '▁sind', '▁im', '▁Frei', 'en', '▁in', '▁der', '▁Nähe', '▁viel', 'er', '▁Bü', 'sche', '.']]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.transforms import SentencePieceTokenizer\n",
    "\n",
    "# using default SPM model by Torch\n",
    "spm_model_path = r\"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model\"\n",
    "sp_tokenizer = SentencePieceTokenizer(spm_model_path)\n",
    "\n",
    "print(sp_tokenizer([eng_sent, de_sent]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we just introduced a new object into our workflow: `transform`. It comes from the `torchtext.transforms` module, which offers a powerful approach to declare text processing pipelines sequentially. More on this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.transforms.SentencePieceTokenizer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.transforms import CLIPTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.transforms.CLIPTokenizer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIPTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SentencePieceTokenizer` is just one of the many tokenizers we can load. Let's take a look at some others.\n",
    "\n",
    "`CLIPTokenizer` treats spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not [see docs](https://pytorch.org/text/stable/transforms.html#cliptokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two</w>',\n",
       " 'young</w>',\n",
       " ',</w>',\n",
       " 'white</w>',\n",
       " 'males</w>',\n",
       " 'are</w>',\n",
       " 'outside</w>',\n",
       " 'near</w>',\n",
       " 'many</w>',\n",
       " 'bushes</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.transforms import CLIPTokenizer\n",
    "\n",
    "MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\n",
    "ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer(\n",
    "    merges_path=MERGES_FILE, \n",
    "    encoder_json_path=ENCODER_FILE,\n",
    "    return_tokens=True\n",
    ")\n",
    "\n",
    "clip_tokenizer(eng_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232k/232k [00:00<00:00, 1.08MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sentence output: ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "\n",
      "Batch sentence output: [['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], ['z', '##wei', 'jung', '##e', 'wei', '##ße', 'manner', 'sin', '##d', 'im', 'fr', '##ei', '##en', 'in', 'der', 'nah', '##e', 'vie', '##ler', 'busch', '##e', '.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext.transforms import BERTTokenizer\n",
    "\n",
    "VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\n",
    "\n",
    "bert_tokenizer = BERTTokenizer(\n",
    "    vocab_path=VOCAB_FILE, \n",
    "    do_lower_case=True, \n",
    "    return_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Single sentence output: {bert_tokenizer(eng_sent)}\")\n",
    "print()\n",
    "print(f\"Batch sentence output: {bert_tokenizer([eng_sent, de_sent])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our vocabulary\n",
    "\n",
    "Using the `torchtext.voacb` module, we can build our `vocab` object by specifying just a few parameters into the constructor:\n",
    "\n",
    "* `ordered_dict` – Ordered Dictionary mapping tokens to their corresponding frequencies\n",
    "* `min_freq` – The minimum frequency needed to include a token in the vocabulary\n",
    "* `specials` – Special symbols to add. The order of supplied tokens will be preserved\n",
    "* `special_first` – Indicates whether to insert symbols at the beginning or at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchtext.vocab.vocab_factory.vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True) -> torchtext.vocab.vocab.Vocab>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import vocab\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# recreating our iterator object\n",
    "train_iter = iter(Multi30k(split='train'))\n",
    "\n",
    "en_counter, de_counter = Counter(), Counter()\n",
    "for paired_sents in train_iter:\n",
    "    de_sent, en_sent = paired_sents\n",
    "    en_counter.update(sp_tokenizer(en_sent))\n",
    "    de_counter.update(sp_tokenizer(de_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁a', 32023), ('.', 27598), ('▁A', 17479), ('▁in', 14947), ('s', 13067)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we could use our counter dictionary as input to the `vocab` constructor, we should use `OrderedDict` since this data structure preserves the order in which keys and values were inserted when iterating over it. Also, if a new entry overwrites an existing entry, then the order of items is left unchanged.\n",
    "\n",
    "Since token frequencies can be relevant when doing machine translation, we should sort our counter by frequencies in descending order and then feed this mapping into `OrderedDict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_en = OrderedDict(sorted(en_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "ordered_de = OrderedDict(sorted(de_counter.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁a', 32023), ('.', 27598), ('▁A', 17479), ('▁in', 14947), ('s', 13067)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ordered_en.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = vocab(\n",
    "    ordered_en, \n",
    "    min_freq=1, \n",
    "    specials=('<BOS>', '<EOS>', '<PAD>', '<unk>')\n",
    ")\n",
    "\n",
    "de_vocab = vocab(\n",
    "    ordered_de, \n",
    "    min_freq=1, \n",
    "    specials=('<BOS>', '<EOS>', '<PAD>', '<unk>')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `vocab` object ([see docs](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab)), you can do things like:\n",
    "*   Get total length of the vocabulary\n",
    "*   Generate mappings - String2Index (stoi) and Index2String (itos)\n",
    "*   A purpose-specific vocabulary which contains words appearing more than N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English vocab is 7367\n",
      "The index of '<BOS>' is 0\n",
      "The token at index 200 is 'a'\n",
      "Special tokens: ['<BOS>', '<EOS>', '<PAD>', '<unk>']\n",
      "5 most common tokens: ['▁a', '.', '▁A', '▁in', 's']\n",
      "5 least common tokens: ['fit', 'ig', 'rah', '▁maj', '▁scroll']\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the English vocab is\", len(en_vocab))\n",
    "en_stoi = en_vocab.get_stoi()\n",
    "print(\"The index of '<BOS>' is\", en_stoi['<BOS>'])\n",
    "en_itos = en_vocab.get_itos()\n",
    "print(f\"The token at index 200 is '{en_itos[200]}'\")\n",
    "print(f\"Special tokens: {en_itos[:4]}\")\n",
    "print(f\"5 most common tokens: {en_itos[4:9]}\")\n",
    "print(f\"5 least common tokens: {en_itos[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `Sequential` to define a text processing pipeline\n",
    "\n",
    "So far, we have looked at how individual components work. But what if we wanted to define them as part of a text processing pipeline? Here's where `transforms.sequential` can be very useful. But before jumping into defining our pipeline, let's take a look at some of the other features in `transforms` which can help us extend our text processing even beyond what we have done so far ([see docs](https://pytorch.org/text/stable/transforms.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.transforms import Sequential, VocabTransform, Truncate, AddToken\n",
    "\n",
    "max_seq_len = 512\n",
    "# assuming we don't have these in our vocab (which we do)\n",
    "bos_idx = 0  \n",
    "eos_idx = 1\n",
    "\n",
    "en_text_transform = Sequential(\n",
    "    SentencePieceTokenizer(spm_model_path),\n",
    "    VocabTransform(en_vocab),\n",
    "    Truncate(max_seq_len - 2),\n",
    "    AddToken(token=bos_idx, begin=True),\n",
    "    AddToken(token=eos_idx, begin=False)\n",
    ")\n",
    "\n",
    "de_text_transform = Sequential(\n",
    "    SentencePieceTokenizer(spm_model_path),\n",
    "    VocabTransform(de_vocab),\n",
    "    Truncate(max_seq_len - 2),\n",
    "    AddToken(token=bos_idx, begin=True),\n",
    "    AddToken(token=eos_idx, begin=False)\n",
    ")\n",
    "\n",
    "def apply_transform(x):\n",
    "    return de_text_transform(x[0]), en_text_transform(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datapipe = Multi30k(split='train')\n",
    "train_datapipe = train_datapipe.map(apply_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's see how our two first paired sentences look after running them through the pipeline:\n",
    "\n",
    "```\n",
    "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
    "Two young, White males are outside near many bushes.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 28, 105, 43, 17, 41, 143, 29, 139, 8, 7, 19, 186, 1541, 24, 1227, 240, 4, 1]\n",
      "[0, 22, 29, 18, 1399, 202, 8, 20, 69, 97, 454, 259, 892, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "train_iter_seq = iter(train_datapipe)\n",
    "num_de, num_en = next(train_iter_seq)\n",
    "print(num_de)\n",
    "print(num_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> ▁Zwei ▁junge ▁weiß e ▁Männer ▁sind ▁im ▁Frei en ▁in ▁der ▁Nähe ▁viel er ▁Bü sche . <EOS>\n",
      "<BOS> ▁Two ▁young , ▁White ▁male s ▁are ▁outside ▁near ▁many ▁bu shes . <EOS>\n"
     ]
    }
   ],
   "source": [
    "# we had only defined mappings on our English vocab\n",
    "de_itos = de_vocab.get_itos()\n",
    "de_stoi = de_vocab.get_stoi()\n",
    "\n",
    "print(\" \".join([de_itos[i] for i in num_de]))\n",
    "print(\" \".join([en_itos[i] for i in num_en]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our text processing pipeline ready to roll, let's see how can we turn it into a trainable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datasets & Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is turn the numericalized data obtained through the processing pipeline and transform it into `torch` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanr\\anaconda3\\lib\\site-packages\\torch\\_jit_internal.py:1282: UserWarning: The inner type of a container is lost when calling torch.jit.isinstance in eager mode. For example, List[int] would become list and therefore falsely return True for List[float] or List[str].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([   0,   28,  105,   43,   17,   41,  143,   29,  139,    8,    7,   19,\n",
       "          186, 1541,   24, 1227,  240,    4,    1]),\n",
       " tensor([   0,   22,   29,   18, 1399,  202,    8,   20,   69,   97,  454,  259,\n",
       "          892,    5,    1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from: https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_stoi['<PAD>']\n",
    "BOS_IDX = de_stoi['<BOS>']\n",
    "EOS_IDX = de_stoi['<EOS>']\n",
    "\n",
    "def data_process(datapipe):\n",
    "  \"\"\"Converts numericalized inputs into Torch tensors.\"\"\"\n",
    "  iter_seq = iter(datapipe)\n",
    "  data = []\n",
    "  for num_de, num_en in iter_seq:\n",
    "    de_tensor_ = torch.tensor(num_de, dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor(num_en, dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_datapipe = Multi30k(split='train').map(apply_transform)\n",
    "\n",
    "train_data = data_process(train_datapipe)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the next step is to generate a data batch using `DataLoader`. This object combines a dataset and a sampler, and provides an iterable over the given dataset ([see docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)).\n",
    "\n",
    "There are a couple of things we need to do first:\n",
    "* Be sure to convert our special tokens `<BOS>` and `<EOS>` into tensors and fit them into the sequence where they belong using `cat`.\n",
    "* Generate fixed-length tensors using padding.\n",
    "\n",
    "The function `generate_batch` takes care of that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "  \"\"\"Generates batch of Torch tensors.\"\"\"\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to instantiate `DataLoader`, which we can do by passing the following parameters into the constructor:\n",
    "\n",
    "* `dataset`: Dataset from which to load the data\n",
    "* `batch_size`: How many samples per batch to load (defaults to 1)\n",
    "* `shuffle`: Reshuffles data at every epoch (defaults to `False`)\n",
    "* `collate_fn`: Callable passed when using a batched loading from a map-style dataset\n",
    "\n",
    "Take a look at the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to explore the full range of parameters you can pass to `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of de_batch: torch.Size([46, 128])\n",
      "Size of en_batch: torch.Size([48, 128])\n"
     ]
    }
   ],
   "source": [
    "train_iter = DataLoader(\n",
    "  train_data, \n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True, \n",
    "  collate_fn=generate_batch\n",
    ")\n",
    "\n",
    "de_batch, en_batch = next(iter(train_iter))\n",
    "print(f\"Size of de_batch: {de_batch.size()}\")\n",
    "print(f\"Size of en_batch: {en_batch.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how one example within the minibatch would look like. Please note that the indeces from this example might look different from the indeces when accessing `train_data[0]` due to having shuffled the data.\n",
    "\n",
    "Also, note that we are getting two 0s and two 1s, which correspond to `<BOS>` and `<EOS>` respectively. This is redundant and is the consequence of using `AddToken` when running our text preprocessing pipeline using `Sequential`, despite having included those tokens inside `special_tokens` when instantiating `vocab`. This was done for demostration purposes, and  **only one of the two approaches should be used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,   15, 1657,   96,   91,   12,    6, 2204,    4,    1,    1,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_batch[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7420e893be28a0404d3f16a3a722ac074c4831bbd966aebbe8043ef702d5e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
